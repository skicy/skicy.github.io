<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
  <head>
   <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
/* Layout and HTML pieces the work of Jon Barron http://www.cs.berkeley.edu/~barron/ */
      a {
      color: #1772d1;
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09227;
      text-decoration:none;
      }
      body,td,th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
      }
    </style>
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <title>Gang Yu, Megvii Face++</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">


    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111692850-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-111692850-1');
</script>


  </head>

  <body>
    <table width="850" border="0" align="center" cellpadding="20">
      <tr>
  <td> 
          <table width="100%" align="center" border="0" cellpadding="10">
            <tr>
              <td width="67%" valign="middle">
    <p align="center"><font size="6">Gang YU (俞刚)</font></p>
    <p align="justify">
                    I am a Research Leader for the Detection Team at <a href="https://megvii.com/">Megvii (Face++)</a>. My research interests focus on the computer vision and artificical intelligence, specifically on the topic of object detection, segmentation, human keypoint, and human action recognition. I obtained PhD from <a href="http://www.ntu.edu.sg/Pages/home.aspx">NTU</a> in 2014 supervised by Prof. <a target="_blank" href="https://www.cse.buffalo.edu/~jsyuan/index.html">Junsong Yuan</a>.
        <br/> <br/>
    </p>
    <p align=center>
      <a href="https://scholar.google.com.sg/citations?user=BJdigYsAAAAJ&hl=en">Google Scholar</a> /
                  <a href="CV.pdf">CV</a>
    </p>
              </td>
              <td width="20%">
              <br>
              <br>
    <img width="210"
                     src="IMGP9957.jpg" 
                     alt="Gang Yu">
        </td>
            </tr> 
          </table>

<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
            <tr>
              <td width="100%" align="left">
                <font size="5">News</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->
                <p>
                  Our team obtained the first place of <strong><a  target="_blank" href="Presentation/instanceSeg_final.pdf">COCO Detecction</a>, <a  target="_blank" href="Presentation/keypoint.pdf">COCO Keypoint Detection</a>, <a  target="_blank" href="Presentation/panoptic.pdf">COCO Panoptic Segmentation, Mapillary Panoptic Segmentation</a></strong> (four Champions) in the <a  target="_blank" href="http://cocodataset.org/workshop/coco-mapillary-eccv-2018.html">COCO + Mapillary Joint Challenge</a>.  <a  target="_blank" href="https://mp.weixin.qq.com/s/K36j2JcTWPV1velUqyy6pA">ChinaMedia 1</a> <a  target="_blank" href="https://mp.weixin.qq.com/s/nL9l7hvG3RG7G7LzCzzvug">ChinaMedia 2</a>(ECCV 2018) .
                  </p>  
                <p>
                  Our team obtained the first place of <strong><a  target="_blank" href="http://wider-challenge.org/">WiderFace Detecction</a></strong> in the <a  target="_blank" href="http://wider-challenge.org/">Wider Challenge</a> (ECCV 2018).
                  </p>  
                <p>
                  Our team obtained the first place of <strong><a  target="_blank" href="https://www.kaggle.com/c/cvpr-2018-autonomous-driving/leaderboard">Video Instance Segmentation</a></strong> [<a  target="_blank" href="WAD/wad_final.pdf">Report</a>][<a  target="_blank" href="WAD/wad_ppt.pdf">Slides</a>] in the <a  target="_blank" href="http://www.wad.ai/index.html">WAD2018 (Workshop on Autonomous Driving)</a> Challenge (CVPR 2018).
                  </p>
                  <p>
                  Our team obtained the first place of <strong><a  target="_blank" href="http://activity-net.org/challenges/2018/tasks/guest_ava.html">AVA</a></strong> [<a  target="_blank" href="AVA/AVA_report.pdf">Report</a>][<a  target="_blank" href="AVA/AVA_slides.pdf">Slides</a>] and second place of <strong><a  target="_blank" href="http://moments.csail.mit.edu/results2018.html">Moments in time</a></strong> [<a  target="_blank" href="http://moments.csail.mit.edu/challenge2018/Megvii.pdf">Report</a>] in the <a  target="_blank" href="http://activity-net.org/challenges/2018/index.html">ActivityNet2018</a> Challenge (CVPR 2018).
                  </p>
                 <p>
                Presentation: <a  target="_blank" href="Presentation/Beyond Mask RCNN and RetinaNet.pdf">Beyond RetinaNet and Mask R-CNN</a>, <a  target="_blank" href="http://www.thejiangmen.com/">Jiangmen (将门)</a>, 2018
                  </p>
                  <p>
                  Presentation: <a  target="_blank" href="Presentation/Intro_Object Detection.pdf">Introduction to Object Detection</a>, PKU & CAS, 2018
                  </p>
                  <p>
                  Our team obtained the first place of <strong><a target="_blank" href="https://places-coco2017.github.io/">COCO 2017 Challenge</strong> (<a target="_blank" href="http://presentations.cocodataset.org/COCO17-Detect-Megvii.pdf">Detection Track</a> & <a target="_blank" href="http://presentations.cocodataset.org/COCO17-Keypoints-Megvii.pdf">Keypoint Track</a>) (ICCV 2017). <br>
 
                  </p>
              </td>
            </tr>
          </table>

 <br><br>


    
    
<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
            <tr>
              <td width="100%" align="left">
                <font size="5">Pre-prints</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->   
              </td>
            </tr>

  <tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1901.00148">Rethinking on Multi-Stage Networks for Human Pose Estimation</a> </b><br>
      Wenbo Li, Zhicheng Wang, Binyi Yin, Qixiang Peng, Yuming Du, Tianzi Xiao, <strong>Gang Yu</strong>, Hongtao Lu, Yichen Wei, Jian Sun<br>                   
    <i>Arxiv</i>, 2019 <br>
    </p>
    </td>
    </tr>

  
  <tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1711.07264.pdf">Light-Head R-CNN: In Defense of Two-Stage Object Detector</a> [<a target="_blank" href="https://github.com/zengarden/light_head_rcnn">Code</a>]</b><br>
      Zeming Li, Chao Peng, <strong>Gang Yu</strong>, Xiangyu Zhang, Yangdong Deng, Jian Sun <br>                   
    <i>Arxiv</i>, 2017 <br>
    </p>
    </td>
    </tr>

  
<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1804.06559.pdf">SFace: An Efficient Network for Face Detection in Large Scale Variations
</a></b><br>
      Jianfeng Wang, Ye Yuan, Boxun Li, <strong>Gang Yu</strong>, Jian Sun<br>                   
    <i>Arxiv</i>, 2018 <br>
    </p>
    </td>
    </tr>

  
<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1711.07246.pdf">Face Attention Network: An Effective Face Detector for the Occluded Faces
</a></b><br>
      Jianfeng Wang, Ye Yuan, <strong>Gang Yu</strong><br>                   
    <i>Arxiv</i>, 2017 <br>
    </p>
    </td>
    </tr>
  
  <tr>
    <td width="100%" align="left">
    <p>
    <b><a  target="_blank" href="https://arxiv.org/pdf/1805.00123.pdf">CrowdHuman: A Benchmark for Detecting Human in a Crowd</a> [<a target="_blank" href="https://sshao0516.github.io/CrowdHuman/">Project</a>]</b><br>
      Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, <strong>Gang Yu</strong>, Xiangyu Zhang, Jian Sun <br>                   
    <i>Arxiv</i>, 2018 <br>
    </p>
    </td>
    </tr>


          </table>

 
<br>
<br>
<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
    <tr>
      <td width="100%" align="left">
                <font size="5">Conference</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->                  
         </td>
       </tr>



    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1903.05027.pdf">An End-to-end Network for Panoptic Segmentation</a> </b><br>
      Huanyu Liu, Chao Peng, Changqian Yu, Jingbo Wang, Xu Liu, <strong>Gang Yu</strong>, Wei Jiang<br>                   
    <i>CVPR</i>, 2019 <br>
    </p>
    </td>
    </tr>




    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="Paper/CVPR2019_TACNET.pdf">TACNet: Transition-Aware Context Network for Spatio-Temporal Action Detection</a> </b><br>
      Lin Song, Shiwei Zhang, <strong>Gang Yu</strong>, Hongbin Sun <br>                   
    <i>CVPR</i>, 2019 <br>
    </p>
    </td>
    </tr>



    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1811.07782.pdf">Modeling Local Geometric Structure of 3D Point Clouds using Geo-CNN</a> </b><br>
      Shiyi Lan, Ruichi Yu, <strong>Gang Yu</strong>, Larry Davis <br>                   
    <i>CVPR</i>, 2019 <br>
    </p>
    </td>
    </tr>

 

    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1903.12473.pdf">Shape Robust Text Detection with Progressive Scale Expansion Network</a> </b><br>
      Wenhai Wang, Xiang Li, Enze Xie, Wenbo Hou, Tong Lu, <strong>Gang Yu</strong>, Shuai Shao <br>                   
    <i>CVPR</i>, 2019 <br>
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1811.08605.pdf">Scene Text Detection with Supervised Pyramid Context Network</a> </b><br>
      Enze Xie, Yuhang Zang, Shuai Shao, <strong>Gang Yu</strong>, Cong Yao,  Guangyao Li <br>                   
    <i>AAAI</i>, 2019 <br>
    </p>
    </td>
    </tr>

    

  
<tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://pdfs.semanticscholar.org/1216/eebb5a407b40eb46596073f0fd229acaea48.pdf">Attention-based Multi-Context Guiding for Few-Shot Semantic Segmentation
</a> </b><br>
      Tao Hu, Pengwan Yang, Chiliang Zhang, <strong>Gang Yu</strong>, Yadong Mu,  Cees Snoek <br>                   
    <i>AAAI</i>, 2019 <br>
    </p>
    </td>
    </tr>
    


    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1804.06215.pdf">DetNet: A Backbone network for Object
Detection
</a> </b><br>
      Zeming Li, Chao Peng, <strong>Gang Yu</strong>, Xiangyu Zhang, Yangdong Deng, Jian Sun <br>                   
    <i>ECCV</i>, 2018 <br>
    </p>
    </td>
    </tr>

  
  
    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1808.00897.pdf">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation
</a> </b><br>
      Changqian Yu, Jingbo Wang,  Chao Peng, Changxin Gao, <strong>Gang Yu</strong>, Nong Sang<br>                   
    <i>ECCV</i>, 2018 <br>
    </p>
    </td>
    </tr>
 
  
     <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ruochen_Fan_Associating_Inter-Image_Salient_ECCV_2018_paper.pdf">Associating Inter-Image Salient Instances forWeakly Supervised Semantic Segmentation
</a> </b><br>
      Ruochen Fan, Qibin Hou, Ming-ming Chen, <strong>Gang Yu</strong>, Ralph R. Martin, Shi-min Hu<br>                   
    <i>ECCV</i>, 2018 <br>
    </p>
    </td>
    </tr>
 
  
    <tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1711.07240.pdf">MegDet: A Large Mini-Batch Object Detector</a></b><br>
      Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, <strong>Gang Yu</strong>, Jian Sun <br>                   
    <i>CVPR</i>, 2018<br>
    </p>
    </td>
    </tr>

<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1711.07319.pdf">Cascaded Pyramid Network for Multi-Person Pose Estimation</a> [<a target="_blank" href="https://github.com/chenyilun95/tf-cpn">Code</a>]</b><br>
      Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, <strong>Gang Yu</strong>, Jian Sun <br>                   
    <i>CVPR</i>, 2018 <br>
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1804.09337.pdf">Learning a Discriminative Feature Network for Semantic Segmentation</a></b><br>
      Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, <strong>Gang Yu</strong>, Nong Sang<br>                   
    <i>CVPR</i>, 2018<br>
    </p>
    </td>
    </tr>

       <tr>
         <td width="100%" valign="top">
    <p>
    <b><a target="_blank" href="Paper/RFCN_plus_plus.pdf">R-FCN++: Towards Accurate Region-based Fully Convolutional Networks for Object Detection</a></b><br>
      Zeming Li, Yilun Chen, <strong>Gang Yu</strong>, Xiangyu Zhang, Jian Sun <br>
    <i>AAAI</i>, 2018 <br>                   
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://arxiv.org/pdf/1703.02719.pdf">Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network</a></b><br>
      Chao Peng, Xiangyu Zhang, <strong>Gang Yu</strong>, Guiming Luo, Jian Sun <br>
    <i>CVPR</i>, 2017 <br>                   
    </p>
    </td>
    </tr>    


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="http://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2015/Fast%20Action%20Proposals%20for%20Human%20Action%20Detection%20and%20Search.pdf">Fast Action Proposals for Human Action Detection and Search</a></b><br>
      <strong>Gang Yu</strong>, Junsong Yuan<br>
    <i>CVPR</i>, 2015 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://dl.dropboxusercontent.com/u/28637011/Paper/RGBD_ActionRecognition.pdf">Discriminative Orderlet Mining For Real-time Recognition of Human-Object Interaction</a></b>   [<a href="https://sites.google.com/site/skicyyu/rgbd_recognition">Project</a>]
    <br><strong>Gang Yu</strong>, Zicheng Liu, Junsong Yuan<br>
    <i>ACCV</i>, 2014 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2014/Scalable%20Forest%20Hashing%20for%20Fast%20Similarity%20Search.pdf">Scalable Forest Hashing for Fast Similarity Search</a></b> 
    <br><strong>Gang Yu</strong>, Junsong Yuan<br>
    <i>ICME</i>, 2014 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://pdfs.semanticscholar.org/b870/3f49d5ac4765b10b5707fea024f9faad931e.pdf">Propagative Hough Voting for Human Activity Recognition</a></b>
    <br><strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu<br>
    <i>ECCV</i>, 2012 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://pdfs.semanticscholar.org/1b38/aac66dbcf13e6d0cc60519b963aac323ef8f.pdf">Randomized Spatial Partition for Scene Recognition</a></b>
    <br>Yuning Jiang, Junsong Yuan, <strong>Gang Yu</strong><br>
    <i>ECCV</i>, 2012 <br>                   
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://pdfs.semanticscholar.org/bbc1/1f20ad5ee2443bdce116e20177b27c96a7e5.pdf">Predicting Human Activities using Spatio-Temporal Structure of Interest Points</a></b>
    <br><strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu<br>
    <i>ACM MM</i>, 2012 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/05995488-CVPR2011.pdf">Unsupervised Random Forest Indexing for Fast Action Search</a></b>
    <br><strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu<br>
    <i>CVPR</i>, 2011<br>                   
    </p>
    </td>
    </tr>
 

    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ACMMM2011-publish.pdf">Real-time HumanAction Search using Random Forest based Hough Voting</a></b>
    <br><strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu<br>
    <i>ACM MM</i>, 2011<br>                   
    </p>
    </td>
    </tr>
      
          </table> 


<br></br>

<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
            <tr>
              <td width="100%" align="left">
                <font size="5">Journal</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->   
              </td>
            </tr>
<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2015/Propagative%20Hough%20Voting%20for%20Human%20Activity%20Detection%20and%20Recognition.pdf">Propagative Hough Voting for Human Activity Detection and Recognition</a></b><br>
    <strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu <br>                   
    <i>IEEE Trans. on Circuits and Systems for Video Technology, Vol.25, Issue 1, pp.87-98</i>, 2014 <br>
    </p>
    </td>
    </tr>


<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="http://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2012/Action%20Search%20by%20Example%20using%20Randomized%20Visual%20Vocabularies.pdf">Action Search by Example using Randomized Visual Vocabularies</a></b><br>
    <strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu <br>                   
    <i>IEEE Trans. on Image Processing, Vol.22, Issue 1, pp. 377-390</i>, 2013 <br>
    </p>
    </td>
    </tr>



<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/05730498-TMM2011.pdf">Fast Action Detection via Discriminative Random Forest Voting and Top-K Subvolume Search</a></b><br>
    <strong>Gang Yu</strong>,  Norberto A., Junsong Yuan, Zicheng Liu <br>                   
    <i>IEEE Trans. on Multimedia, Vol.13, Issue 3, pp. 507-517</i>, 2013 <br>
    </p>
    </td>
    </tr>


          </table>






<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
            <tr>
              <td width="100%" align="left">
                <font size="5">Book</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->   
              </td>
            </tr>
<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2015/Propagative%20Hough%20Voting%20for%20Human%20Activity%20Detection%20and%20Recognition.pdf">Human Action Analysis with Randomized Trees</a></b><br>
    <strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu <br>                   
    <i>SpringerBriefs, Springer</i>, 2014 <br>
    </p>
    </td>
    </tr>
    </table>


<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
            <tr>
              <td width="100%" align="left">
                <font size="5">Links</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->   
              </td>
            </tr>
<tr>
    <td width="100%" align="left">
    <p>
    Team member: <a target='_blank' href='https://pcsci.github.io'>Chao Peng (彭超)</a>, <a target='_blank' href='http://www.zemingli.com/'>Zeming Li (黎泽明)</a>, <a target='_blank' href='http://changqianyu.me/'>Changqian Yu (余昌黔)</a>, <a target='_blank' href='https://www.sshao.com/'>Shuai Shao (邵帅)</a>  
    </p>
    </td>
    </tr>
    </table>
 
  
