<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
  <head>
   <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
/* Layout and HTML pieces the work of Jon Barron http://www.cs.berkeley.edu/~barron/ */
      a {
      color: #1772d1;
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09227;
      text-decoration:none;
      }
      body,td,th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
      }
    </style>
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <title>Gang Yu</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">


    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111692850-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-111692850-1');
</script>


  </head>

  <body>
    <table width="850" border="0" align="center" cellpadding="20">
      <tr>
  <td> 
          <table width="100%" align="center" border="0" cellpadding="10">
            <tr>
              <td width="67%" valign="middle">
    <p align="center"><font size="6">Gang YU (俞刚)</font></p>
    <p align="justify">
                    I am a Principal Research Scientist at <a href="https://www.stepfun.com/">StepFun (阶跃星辰)</a>. My research interests focus on the computer vision and artificical intelligence, specifically on the topic of generative AI,  object detection, segmentation, human keypoint, human action recognition, and 3D reconstruction. I obtained PhD from <a href="http://www.ntu.edu.sg/Pages/home.aspx">NTU</a> in 2014 supervised by Prof. <a target="_blank" href="https://www.cse.buffalo.edu/~jsyuan/index.html">Junsong Yuan</a>. Before joining StepFun, I worked as a research director at Tencent  <a href="https://www.tencent.com/"></a>  for four years and another five years at <a href="https://www.megvii.com/en/">Megvii (Face++)</a>.
        <br/> <br/>
    </p>
    <p align=center>
      <a target="_blank" href="https://scholar.google.com.sg/citations?user=BJdigYsAAAAJ&hl=en">Google Scholar</a> /
                  <a target="_blank" href="CV.pdf">CV</a> /   <a target="_blank" href="https://zhuanlan.zhihu.com/c_1065911842173468672">Zhihu</a>
    </p>
              </td>
              <td width="20%">
              <br>
              <br>
    <img width="210"
                     src="IMGP9957.jpg" 
                     alt="Gang Yu">
        </td>
            </tr> 
          </table>

<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
            <tr>
              <td width="100%" align="left">
                <font size="5">News</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->
                <p>
                  Two papers accepted by <a  target="_blank" href="https://eccv.ecva.net/Conferences/2024">ECCV2024</a>.
                </p>
                <p>
                  Two papers accepted by <a  target="_blank" href="https://cvpr.thecvf.com/Conferences/2024">CVPR2024</a>.
                </p>
                <p>
                  I will serve as an Innovation Program (Industry) Chair for <a  target="_blank" href="https://2024.ieeeicme.org/">ICME2024</a>.
                </p>
                <p>
                  I will serve as an Area Chair for <a  target="_blank" href="https://cvpr.thecvf.com/Conferences/2024">CVPR2024</a>.
                </p>
                <p>
                  Three papers accepted by <a  target="_blank" href="https://nips.cc/">NeurIPS2023</a>.
                </p>
                <p>
                  Three papers accepted by <a  target="_blank" href="https://iccv2023.thecvf.com/">ICCV2023</a>.
                </p>
                <p>
                  Three papers accepted by <a  target="_blank" href="https://cvpr2023.thecvf.com/">CVPR2023</a>.
                </p>
                <p>
                  We have organized a tutorial <strong><a  target="_blank" href="https://embedded-dl-lab.github.io/mobile-visual-analytics/">Mobile Visual Analytics</a></strong>  in <a  target="_blank" href="https://cvpr2021.thecvf.com/">CVPR2021</a>.
                 </p>
                 <p>
                  Our team obtained the first place of <strong><a  target="_blank" href="https://ai-benchmark.com/workshops/mai/2021/">Mobile AI Challenge</a></strong> in the <a  target="_blank" href="https://arxiv.org/pdf/2105.08630.pdf">Depth Estimation Challenge</a> (CVPR 2021).
                  </p>  

                <p>
                  We have organized a tutorial <strong><a  target="_blank" href="http://2019.ieeeicip.org/?action=page4&id=5">Human Pose Estimation and Action Recognition</a></strong> [<a  target="_blank" href="Presentation/ICIP2019/2019.09 ICIP skeleton.pdf">Skeleton</a>, <a  target="_blank" href="Presentation/ICIP2019/2019.09 ICIP Action.pdf">Action</a>] (ICIP 2019).
                </p>
                <p>
                  We have organized a tutorial <strong><a  target="_blank" href="http://www.icme2019.org/conf_tutorials">Object Detection in Recent Three Years</a></strong> [<a  target="_blank" href="Presentation/ICME2019_tutorial_slides/GangYU-2019.07.08_Object Detection.pdf">Detection</a>, <a  target="_blank" href="Presentation/ICME2019_tutorial_slides/automl for detection-share.pdf">AutoML</a>, <a  target="_blank" href="Presentation/ICME2019_tutorial_slides/weixs_ICME 2019-compressed.pdf">Fine-Grained</a>] (ICME 2019).
                </p>
                <p>
                  We organized the <strong><a  target="_blank" href="http://www.objects365.org/workshop2019.html">Detection In the Wild (DIW2019) Challenge</a></strong>  in <a  target="_blank" href="http://cvpr2019.thecvf.com/">CVPR2019</a>.
                 </p>
                <p>
                  Our team obtained the first place of <strong><a  target="_blank" href="https://www.nuscenes.org/object-detection?externalData=no&mapData=no&modalities=Lidar">nuScenes 3D Detection</a></strong> and <strong><a  target="_blank" href="https://outreach.didichuxing.com/d2city/sort">BDD100K & D²-City Detection Domain Adaptation</a></strong> in the <a  target="_blank" href="https://sites.google.com/view/wad2019/challenge">Workshop on Autonomous Driving</a> (CVPR 2019).
                </p>
                <p>
                  Our team obtained the first place of <strong><a  target="_blank" href="Presentation/instanceSeg_final.pdf">COCO Detecction</a>, <a  target="_blank" href="Presentation/keypoint.pdf">COCO Keypoint Detection</a>, <a  target="_blank" href="Presentation/panoptic.pdf">COCO Panoptic Segmentation, Mapillary Panoptic Segmentation</a></strong> (four Champions) in the <a  target="_blank" href="http://cocodataset.org/workshop/coco-mapillary-eccv-2018.html">COCO + Mapillary Joint Challenge</a>.  <a  target="_blank" href="https://mp.weixin.qq.com/s/K36j2JcTWPV1velUqyy6pA">ChinaMedia 1</a> <a  target="_blank" href="https://mp.weixin.qq.com/s/nL9l7hvG3RG7G7LzCzzvug">ChinaMedia 2</a>(ECCV 2018) .
                  </p>  
                <p>
                  Our team obtained the first place of <strong><a  target="_blank" href="http://wider-challenge.org/">WiderFace Detecction</a></strong> in the <a  target="_blank" href="http://wider-challenge.org/">Wider Challenge</a> (ECCV 2018).
                  </p>  
                <p>
                  Our team obtained the first place of <strong><a  target="_blank" href="https://www.kaggle.com/c/cvpr-2018-autonomous-driving/leaderboard">Video Instance Segmentation</a></strong> [<a  target="_blank" href="WAD/wad_final.pdf">Report</a>][<a  target="_blank" href="WAD/wad_ppt.pdf">Slides</a>] in the <a  target="_blank" href="http://www.wad.ai/index.html">WAD2018 (Workshop on Autonomous Driving)</a> Challenge (CVPR 2018).
                  </p>
                  <p>
                  Our team obtained the first place of <strong><a  target="_blank" href="http://activity-net.org/challenges/2018/tasks/guest_ava.html">AVA</a></strong> [<a  target="_blank" href="AVA/AVA_report.pdf">Report</a>][<a  target="_blank" href="AVA/AVA_slides.pdf">Slides</a>] and second place of <strong><a  target="_blank" href="http://moments.csail.mit.edu/results2018.html">Moments in time</a></strong> [<a  target="_blank" href="http://moments.csail.mit.edu/challenge2018/Megvii.pdf">Report</a>] in the <a  target="_blank" href="http://activity-net.org/challenges/2018/index.html">ActivityNet2018</a> Challenge (CVPR 2018).
                  </p>
                 <p>
                Presentation: <a  target="_blank" href="Presentation/Beyond Mask RCNN and RetinaNet.pdf">Beyond RetinaNet and Mask R-CNN</a>, <a  target="_blank" href="http://www.thejiangmen.com/">Jiangmen (将门)</a>, 2018
                  </p>
                  <p>
                  Presentation: <a  target="_blank" href="Presentation/Intro_Object Detection.pdf">Introduction to Object Detection</a>, PKU & CAS, 2018
                  </p>
                  <p>
                  Our team obtained the first place of <strong><a target="_blank" href="https://places-coco2017.github.io/">COCO 2017 Challenge</strong> (<a target="_blank" href="http://presentations.cocodataset.org/COCO17-Detect-Megvii.pdf">Detection Track</a> & <a target="_blank" href="http://presentations.cocodataset.org/COCO17-Keypoints-Megvii.pdf">Keypoint Track</a>) (ICCV 2017). <br>
 
                  </p>
              </td>
            </tr>
          </table>

 <br><br>
    
<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
            <tr>
              <td width="100%" align="left">
                <font size="5">Pre-prints</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->   
              </td>
            </tr>

 <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank"  href="https://arxiv.org/abs/2312.13771">AppAgent: Multimodal Agents as Smartphone Users</a> </b><br>
      Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, <strong>Gang Yu</strong><br>          
    <i>Arxiv</i>, 2023 <br>
    </p>
    </td>
  </tr>

  <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank"  href="https://arxiv.org/abs/2311.16483">ChartLlama: A Multimodal LLM for Chart Understanding and Generation</a> </b><br>
      Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, <strong>Gang Yu</strong>, Bin Fu, Hanwang Zhang<br>          
    <i>Arxiv</i>, 2023 <br>
    </p>
    </td>
  </tr>



  <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank"  href="https://arxiv.org/abs/2312.02663">FaceStudio: Put Your Face Everywhere in Seconds</a> </b><br>
      Yuxuan Yan, Chi Zhang, Rui Wang, Yichao Zhou, Gege Zhang, Pei Cheng, <strong>Gang Yu</strong>, Bin Fu<br>          
    <i>Arxiv</i>, 2023 <br>
    </p>
    </td>
  </tr>




 <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank"  href="https://arxiv.org/abs/2311.17618">ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model</a> </b><br>
      Fukun Yin, Xin Chen, Chi Zhang, Biao Jiang, Zibo Zhao, Jiayuan Fan, <strong>Gang Yu</strong>, Taihao Li, Tao Chen<br>          
    <i>Arxiv</i>, 2023 <br>
    </p>
    </td>
  </tr>



  

  

    
    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank"  href="https://arxiv.org/pdf/2106.03650.pdf">Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer</a> </b><br>
      Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, <strong>Gang Yu</strong>, Bin Fu<br>          
    <i>Arxiv</i>, 2021 <br>
    </p>
    </td>
    </tr>

</table>


 
<br>
<br>
<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
    <tr>
      <td width="100%" align="left">
                <font size="5">Conference</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->                  
         </td>
       </tr>

       <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank"  href="https://arxiv.org/abs/2312.10763">M3DBench: Towards Omni 3D Assistant with Interleaved Multi-modal Instructions</a> </b><br>
          Mingsheng Li, Xin Chen, Chi Zhang, Sijin Chen, Hongyuan Zhu, Fukun Yin, <strong>Gang Yu</strong>, Tao Chen<br>          
        <i>ECCV</i>, 2024 <br>
        </p>
        </td>
      </tr>

       <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank"  href="https://arxiv.org/abs/2404.01700">MotionChain: Conversational Motion Controllers via Multimodal Prompts
        </a> </b><br>
        Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li, <strong>Gang Yu</strong><br>, Jiayuan Fan          
        <i>ECCV</i>, 2024 <br>
        </p>
        </td>
      </tr>

      <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank"  href="https://arxiv.org/abs/2403.11469">Generative Motion Stylization of Cross-structure Characters within Canonical Motion Space
        </a> </b><br>
        Jiaxu Zhang, Xin Chen, <strong>Gang Yu</strong><br>, Zhigang Tu       
        <i>ACM Multimedia</i>, 2024 <br>
        </p>
        </td>
      </tr>


      <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank"  href="https://arxiv.org/abs/2308.10253v2">Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data</a> </b><br>
        Yanda Li, Chi Zhang, <strong>Gang Yu</strong>, Wanqi Yang, Zhibin Wang, BIN FU, Guosheng Lin, Chunhua Shen, Ling Chen, Yunchao Wei  
        <i>ACL Findings</i>, 2024 <br>
        </p>
        </td>
      </tr>

       <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank"  href="https://arxiv.org/abs/2404.01700">MotionChain: Conversational Motion Controllers via Multimodal Prompts
        </a> </b><br>
        Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li, <strong>Gang Yu</strong><br>, Jiayuan Fan          
        <i>ECCV</i>, 2024 <br>
        </p>
        </td>
      </tr>


       <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank"  href="https://arxiv.org/abs/2312.13913">Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models</a> </b><br>
          Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, <strong>Gang Yu</strong><br>          
        <i>CVPR</i>, 2024 <br>
        </p>
        </td>
      </tr>



       <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank"  href="https://arxiv.org/abs/2312.13913">Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models</a> </b><br>
          Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, <strong>Gang Yu</strong><br>          
        <i>CVPR</i>, 2024 <br>
        </p>
        </td>
      </tr>

      <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank"  href="https://arxiv.org/abs/2311.18651">LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning</a> </b><br>
          Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, <strong>Gang Yu</strong>, Hao Fei, Hongyuan Zhu, Jiayuan Fan, Tao Chen<br>          
        <i>CVPR</i>, 2024 <br>
        </p>
        </td>
      </tr>

      <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank"  href="https://arxiv.org/abs/2310.12678">TapMo: Shape-aware Motion Generation of Skeleton-free Characters</a> </b><br>
          Jiaxu Zhang, Shaoli Huang, Zhigang Tu, Xin Chen, Xiaohang Zhan, <strong>Gang Yu</strong>, Ying Shan<br>          
        <i>ICLR</i>, 2024 <br>
        </p>
        </td>
      </tr>

      <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank"  href="https://arxiv.org/abs/2308.11473"> IT3D: Improved Text-to-3D Generation with Explicit View Synthesis</a> </b><br>
        Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, <strong>Gang Yu</strong>, Lei Yang, Guosheng Lin
        <i>AAAI</i>, 2024 <br>
        </p>
        </td>
      </tr>


      <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank"  href="https://ojs.aaai.org/index.php/AAAI/article/view/28481"> PM-INR: Prior-Rich Multi-Modal Implicit Large-Scale Scene Neural
          Representation</a> </b><br>
          Yiying Yang, Fukun Yin, Wen Liu, Jiayuan Fan, Xin Chen, <strong>Gang Yu</strong> Tao Chen     
        <i>AAAI</i>, 2024 <br>
        </p>
        </td>
      </tr>
      
       <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank" href="https://arxiv.org/abs/2306.14795">MotionGPT: Human Motion as a Foreign Language</a> </b>
        <br>
        Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, <strong>Gang Yu</strong>, Tao Chen <br>                   
        <i>NeurIPS</i>, 2023<br>
        </p>
        </td>
  </tr>


  <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank" href="https://arxiv.org/abs/2306.17115">Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation</a> </b>
        <br>
        Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, <strong>Gang Yu</strong>, Shenghua Gao<br>                   
        <i>NeurIPS</i>, 2023<br>
        </p>
        </td>
  </tr>
  
  <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank" href="">PDF: Point Diffusion Implicit Function for Large-scale Scene Neural Representation</a> </b>
        <br>
        Yuhan Ding, Fukun Yin, Jiayuan Fan, Hui Li, Xin Chen, Wen Liu, Chongshan Lu, <strong>Gang Yu</strong>, Tao Chen <br>                   
        <i>NeurIPS</i>, 2023<br>
        </p>
        </td>
  </tr>



   <tr>
        <td width="100%" align="left">
        <p>
        <b><a target="_blank" href="https://arxiv.org/abs/2301.06782">A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction</a> </b>
        <br>
        Chongshan Lu, Fukun Yin, Xin Chen, Wen Liu, Tao Chen, <strong>Gang Yu</strong>, Jiayuan Fan <br>                   
        <i>ICCV</i>, 2023<br>
        </p>
        </td>
    </tr>

    <tr>
      <td width="100%" align="left">
      <p>
      <b><a target="_blank" href="https://arxiv.org/abs/2307.10984">Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image</a> </b>
      <br>
      Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Xiaozhi Chen, Kaixuan Wang，<strong>Gang Yu</strong>, Chunhua Shen<br>                   
      <i>ICCV</i>, 2023<br>
      </p>
      </td>
  </tr>

  <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="">Robust Geometry-Preserving Depth Estimation Using Differentiable Rendering</a> </b>
    <br>
    Chi Zhang, Wei Yin, <strong>Gang Yu</strong>, Zhibin Wang, Tao Chen, Bin Fu, Tianyi Zhou, Chunhua Shen<br>                   
    <i>ICCV</i>, 2023<br>
    </p>
    </td>
</tr>


       <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/abs/2212.04048">Executing your Commands via Motion Diffusion in Latent Space</a> </b>
    <br>
Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, BIN FU, Tao Chen, <strong>Gang Yu</strong> <br>                   
    <i>CVPR</i>, 2023<br>
    </p>
    </td>
</tr>



<tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/abs/2306.02763">STAR Loss: Reducing Semantic Ambiguity in Facial Landmark Detection</a> </b>
    <br>
Zhenglin Zhou, Huaxia Li, Hong Liu, Nanyang Wang,  <strong>Gang Yu</strong> , Rongrong Ji <br>                   
    <i>CVPR</i>, 2023<br>
    </p>
    </td>
  </tr>

<tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/abs/2301.02508">End-to-End 3D Dense Captioning with Vote2Cap-DETR</a> </b>
    <br>
Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Tao Chen, <strong>Gang Yu</strong> <br>                   
    <i>CVPR</i>, 2023<br>
    </p>
    </td>
</tr>




  <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/2303.00298.pdf">Capturing the motion of every joint: 3D human pose and shape estimation with independent tokens</a> </b>
    <br>
Sen Yang, Wen Heng, Gang Liu, Guozhong Luo, Wankou Yang, <strong>Gang Yu</strong> <br>                   
    <i>ICLR</i>, 2023<br>
    </p>
    </td>
  </tr>


  <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/2301.13156.pdf">SeaFormer: Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation</a> </b>
    <br>
Qiang Wan, Zilong Huang, Jiachen Lu, <strong>Gang Yu</strong>,  Li Zhang <br>                   
    <i>ICLR</i>, 2023<br>
    </p>
    </td>
  </tr>


    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/2210.09670.pdf">Hierarchical Normalization for Robust Monocular Depth Estimation</a> </b><br>
Chi Zhang, Wei Yin, Zhibin Wang, <strong>Gang Yu</strong>, Bin Fu, Chunhua Shen <br>                   
    <i>NeurIPS</i>, 2022<br>
    </p>
    </td>
    </tr>
  
    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/2210.11170.pdf">Coordinates Are NOT Lonely - Codebook Prior Helps Implicit Neural 3D Representations</a> </b><br>
Fukun Yin, Wen Liu, Zilong Huang, Pei Cheng, Tao Chen, <strong>Gang Yu</strong> <br>                   
    <i>NeurIPS</i>, 2022<br>
    </p>
    </td>
    </tr>
  
    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/2209.08790.pdf">D&D: Learning Human Dynamics from Dynamic Camera</a> </b><br>
Jiefeng Li, Siyuan Bian, Chao Xu, Gang Liu, <strong>Gang Yu</strong>, Cewu Lu <br>                   
    <i>ECCV</i>, 2022 (ORAL)<br>
    </p>
    </td>
    </tr>
  
  
    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/2204.05525.pdf">TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation</a> </b><br>
Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu Liu, <strong>Gang Yu</strong>, Chunhua Shen <br>                   
    <i>CVPR</i>, 2022<br>
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/2108.13098.pdf">Object-aware Long-short-range Spatial Alignment for Few-Shot Fine-Grained Image Classification</a> </b><br>
Yike Wu, Bo Zhang, <strong>Gang Yu</strong>, Weixi Zhang, Bin Wang, Tao Chen, Jiayuan Fan<br>                   
    <i>ACM MM</i>, 2021<br>
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/2111.13010.pdf">Attribute-specific Control Units in StyleGAN for Fine-grained Image Manipulation
</a> </b><br>
Rui Wang, Jian Chen, <strong>Gang Yu</strong>, Li Sun, Changqian Yu, Changxin Gao, Nong Sang<br>                   
    <i>ACM MM</i>, 2021<br>
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/2003.00482">State-Aware Tracker for Real-Time Video Object Segmentation</a> </b><br>
Xi Chen, Zuoxin Li, Ye Yuan, <strong>Gang Yu</strong>, Jian-Xin Shen, Donglian Qi <br>                   
    <i>CVPR</i>, 2020 <br>
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/2003.08177">High-Order Information Matters: Learning Relation and Topology for Occluded Person Re-Identification</a> </b><br>
Guan'an Wang, Shuo Yang, Huanyu Liu, Zhicheng Wang, Yang Yang, Shuliang Wang, <strong>Gang Yu</strong>, Erjin Zhou, Jian Sun<br>                   
    <i>CVPR</i>, 2020 <br>
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/abs/2004.01547">Context Prior for Scene Segmentation</a> </b><br>
Changqian Yu, Jingbo Wang, Changxin Gao,  <strong>Gang Yu</strong>, Chunhua Shen, Nong Sang<br>                   
    <i>CVPR</i>, 2020 <br>
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1911.06188">SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines</a> </b><br>
Yinda Xu, Zeyu Wang, Zuoxin Li,  Ye Yuan, <strong>Gang Yu</strong><br>                   
    <i>AAAI</i>, 2020 <br>
    </p>
    </td>
    </tr>

 
    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1909.12513">Learnable Tree Filter for Structure-preserving Feature Transform</a> </b><br>
      Lin Song, Yanwei Li, Zeming Li,  <strong>Gang Yu</strong>, Hongbin Sun, Jian Sun, Nanning Zheng<br>                   
    <i>NIPS</i>, 2019 <br>
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1903.11752.pdf">ThunderNet: Towards Real-time Generic Object Detection</a> </b><br>
      Zheng Qin, Zeming Li, Zhaoning Zhang, Yiping Bao, <strong>Gang Yu</strong>, Yuxing Peng, Jian Sun<br>                   
    <i>ICCV</i>, 2019 <br>
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://128.84.21.199/pdf/1908.05900.pdf">Efficient and Accurate Arbitrary-Shaped Text Detection with Pixel Aggregation Network</a> </b><br>Wenhai Wang, Enze Xie, Xiaoge Song, Yuhang Zang, Wenjia Wang, Tong Lu, <strong>Gang Yu</strong>, Chunhua Shen<br> 
    <i>ICCV</i>, 2019 <br>
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="Paper/Objects365.pdf">Objects365: A Large-scale, High-quality Dataset for Object Detection</a> </b><br>Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng,  <strong>Gang Yu</strong>, Jing Li, Xiangyu Zhang, Jian Sun<br> 
    <i>ICCV</i>, 2019 <br>
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1903.05027.pdf">An End-to-end Network for Panoptic Segmentation</a> </b><br>
      Huanyu Liu, Chao Peng, Changqian Yu, Jingbo Wang, Xu Liu, <strong>Gang Yu</strong>, Wei Jiang<br>                   
    <i>CVPR</i>, 2019 <br>
    </p>
    </td>
    </tr>




    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="Paper/CVPR2019_TACNET.pdf">TACNet: Transition-Aware Context Network for Spatio-Temporal Action Detection</a> </b><br>
      Lin Song, Shiwei Zhang, <strong>Gang Yu</strong>, Hongbin Sun <br>                   
    <i>CVPR</i>, 2019 <br>
    </p>
    </td>
    </tr>



    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1811.07782.pdf">Modeling Local Geometric Structure of 3D Point Clouds using Geo-CNN</a> </b><br>
      Shiyi Lan, Ruichi Yu, <strong>Gang Yu</strong>, Larry Davis <br>                   
    <i>CVPR</i>, 2019 <br>
    </p>
    </td>
    </tr>

 

    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1903.12473.pdf">Shape Robust Text Detection with Progressive Scale Expansion Network</a> </b><br>
      Wenhai Wang, Xiang Li, Enze Xie, Wenbo Hou, Tong Lu, <strong>Gang Yu</strong>, Shuai Shao <br>                   
    <i>CVPR</i>, 2019 <br>
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1811.08605.pdf">Scene Text Detection with Supervised Pyramid Context Network</a> </b><br>
      Enze Xie, Yuhang Zang, Shuai Shao, <strong>Gang Yu</strong>, Cong Yao,  Guangyao Li <br>                   
    <i>AAAI</i>, 2019 <br>
    </p>
    </td>
    </tr>

    

  
<tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://pdfs.semanticscholar.org/1216/eebb5a407b40eb46596073f0fd229acaea48.pdf">Attention-based Multi-Context Guiding for Few-Shot Semantic Segmentation
</a> </b><br>
      Tao Hu, Pengwan Yang, Chiliang Zhang, <strong>Gang Yu</strong>, Yadong Mu,  Cees Snoek <br>                   
    <i>AAAI</i>, 2019 <br>
    </p>
    </td>
    </tr>
    


    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1804.06215.pdf">DetNet: A Backbone network for Object
Detection
</a> </b><br>
      Zeming Li, Chao Peng, <strong>Gang Yu</strong>, Xiangyu Zhang, Yangdong Deng, Jian Sun <br>                   
    <i>ECCV</i>, 2018 <br>
    </p>
    </td>
    </tr>

  
  
    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1808.00897.pdf">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation
</a> </b><br>
      Changqian Yu, Jingbo Wang,  Chao Peng, Changxin Gao, <strong>Gang Yu</strong>, Nong Sang<br>                   
    <i>ECCV</i>, 2018 <br>
    </p>
    </td>
    </tr>
 
  
     <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ruochen_Fan_Associating_Inter-Image_Salient_ECCV_2018_paper.pdf">Associating Inter-Image Salient Instances forWeakly Supervised Semantic Segmentation
</a> </b><br>
      Ruochen Fan, Qibin Hou, Ming-ming Chen, <strong>Gang Yu</strong>, Ralph R. Martin, Shi-min Hu<br>                   
    <i>ECCV</i>, 2018 <br>
    </p>
    </td>
    </tr>
 
  
    <tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1711.07240.pdf">MegDet: A Large Mini-Batch Object Detector</a></b><br>
      Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, <strong>Gang Yu</strong>, Jian Sun <br>                   
    <i>CVPR</i>, 2018<br>
    </p>
    </td>
    </tr>

<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1711.07319.pdf">Cascaded Pyramid Network for Multi-Person Pose Estimation</a> [<a target="_blank" href="https://github.com/chenyilun95/tf-cpn">Code</a>]</b><br>
      Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, <strong>Gang Yu</strong>, Jian Sun <br>                   
    <i>CVPR</i>, 2018 <br>
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1804.09337.pdf">Learning a Discriminative Feature Network for Semantic Segmentation</a></b><br>
      Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, <strong>Gang Yu</strong>, Nong Sang<br>                   
    <i>CVPR</i>, 2018<br>
    </p>
    </td>
    </tr>

       <tr>
         <td width="100%" valign="top">
    <p>
    <b><a target="_blank" href="Paper/RFCN_plus_plus.pdf">R-FCN++: Towards Accurate Region-based Fully Convolutional Networks for Object Detection</a></b><br>
      Zeming Li, Yilun Chen, <strong>Gang Yu</strong>, Xiangyu Zhang, Jian Sun <br>
    <i>AAAI</i>, 2018 <br>                   
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://arxiv.org/pdf/1703.02719.pdf">Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network</a></b><br>
      Chao Peng, Xiangyu Zhang, <strong>Gang Yu</strong>, Guiming Luo, Jian Sun <br>
    <i>CVPR</i>, 2017 <br>                   
    </p>
    </td>
    </tr>    


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="http://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2015/Fast%20Action%20Proposals%20for%20Human%20Action%20Detection%20and%20Search.pdf">Fast Action Proposals for Human Action Detection and Search</a></b><br>
      <strong>Gang Yu</strong>, Junsong Yuan<br>
    <i>CVPR</i>, 2015 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://dl.dropboxusercontent.com/u/28637011/Paper/RGBD_ActionRecognition.pdf">Discriminative Orderlet Mining For Real-time Recognition of Human-Object Interaction</a></b>   [<a href="https://sites.google.com/site/skicyyu/rgbd_recognition">Project</a>]
    <br><strong>Gang Yu</strong>, Zicheng Liu, Junsong Yuan<br>
    <i>ACCV</i>, 2014 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2014/Scalable%20Forest%20Hashing%20for%20Fast%20Similarity%20Search.pdf">Scalable Forest Hashing for Fast Similarity Search</a></b> 
    <br><strong>Gang Yu</strong>, Junsong Yuan<br>
    <i>ICME</i>, 2014 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://pdfs.semanticscholar.org/b870/3f49d5ac4765b10b5707fea024f9faad931e.pdf">Propagative Hough Voting for Human Activity Recognition</a></b>
    <br><strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu<br>
    <i>ECCV</i>, 2012 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://pdfs.semanticscholar.org/1b38/aac66dbcf13e6d0cc60519b963aac323ef8f.pdf">Randomized Spatial Partition for Scene Recognition</a></b>
    <br>Yuning Jiang, Junsong Yuan, <strong>Gang Yu</strong><br>
    <i>ECCV</i>, 2012 <br>                   
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://pdfs.semanticscholar.org/bbc1/1f20ad5ee2443bdce116e20177b27c96a7e5.pdf">Predicting Human Activities using Spatio-Temporal Structure of Interest Points</a></b>
    <br><strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu<br>
    <i>ACM MM</i>, 2012 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/05995488-CVPR2011.pdf">Unsupervised Random Forest Indexing for Fast Action Search</a></b>
    <br><strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu<br>
    <i>CVPR</i>, 2011<br>                   
    </p>
    </td>
    </tr>
 

    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ACMMM2011-publish.pdf">Real-time HumanAction Search using Random Forest based Hough Voting</a></b>
    <br><strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu<br>
    <i>ACM MM</i>, 2011<br>                   
    </p>
    </td>
    </tr>
      
          </table> 


<br></br>

<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
  <tr>
    <td width="100%" align="left">
      <font size="5">Journal</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->   
    </td>
  </tr>

  <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/2406.11689">Lightweight Model Pre-Training Via Language Guided Knowledge Distillation</a></b><br>
    Mingsheng Li, Lin Zhang, Mingzhen Zhu, Zilong Huang, <strong>Gang Yu</strong>, Jiayuan Fan, Tao Chen<br>                   
    <i>IEEE Trans. on Multimedia</i>, 2024<br>
    </p>
    </td>
  </tr>
    
    
    
    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/2309.02999">Vote2cap-detr++: Decoupling localization and describing for end-to-end 3d dense captioning</a></b><br>
    Sijin Chen, Hongyuan Zhu, Mingsheng Li, Xin Chen, Peng Guo, Yinjie Lei, <strong>Gang Yu</strong>, Taihao Li, Tao Chen<br>                   
    <i>IEEE Trans. on Pattern Analysis and Machine Intelligence</i>, 2024<br>
    </p>
    </td>
    </tr>
    
    
    <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/2004.02147.pdf">BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation</a></b><br>
    Changqian Yu, Changxin Gao, Jingbo Wang, <strong>Gang Yu</strong>, Chunhua Shen, Nong Sang<br>                   
    <i>International Journal of Computer Vision</i>, 2021<br>
    </p>
    </td>
    </tr>


<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2015/Propagative%20Hough%20Voting%20for%20Human%20Activity%20Detection%20and%20Recognition.pdf">Propagative Hough Voting for Human Activity Detection and Recognition</a></b><br>
    <strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu <br>                   
    <i>IEEE Trans. on Circuits and Systems for Video Technology, Vol.25, Issue 1, pp.87-98</i>, 2014 <br>
    </p>
    </td>
    </tr>


<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="http://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2012/Action%20Search%20by%20Example%20using%20Randomized%20Visual%20Vocabularies.pdf">Action Search by Example using Randomized Visual Vocabularies</a></b><br>
    <strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu <br>                   
    <i>IEEE Trans. on Image Processing, Vol.22, Issue 1, pp. 377-390</i>, 2013 <br>
    </p>
    </td>
    </tr>



<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/05730498-TMM2011.pdf">Fast Action Detection via Discriminative Random Forest Voting and Top-K Subvolume Search</a></b><br>
    <strong>Gang Yu</strong>,  Norberto A., Junsong Yuan, Zicheng Liu <br>                   
    <i>IEEE Trans. on Multimedia, Vol.13, Issue 3, pp. 507-517</i>, 2013 <br>
    </p>
    </td>
    </tr>


</table>






<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
            <tr>
              <td width="100%" align="left">
                <font size="5">Book</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->   
              </td>
            </tr>
<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2015/Propagative%20Hough%20Voting%20for%20Human%20Activity%20Detection%20and%20Recognition.pdf">Human Action Analysis with Randomized Trees</a></b><br>
    <strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu <br>                   
    <i>SpringerBriefs, Springer</i>, 2014 <br>
    </p>
    </td>
    </tr>
    </table>
      
      
