<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
  <head>
   <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
/* Layout and HTML pieces the work of Jon Barron http://www.cs.berkeley.edu/~barron/ */
      a {
      color: #1772d1;
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09227;
      text-decoration:none;
      }
      body,td,th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
      }
    </style>
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <title>Gang Yu, Megvii Face++</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">


    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111692850-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-111692850-1');
</script>


  </head>

  <body>
    <table width="850" border="0" align="center" cellpadding="20">
      <tr>
  <td> 
          <table width="100%" align="center" border="0" cellpadding="10">
            <tr>
              <td width="67%" valign="middle">
    <p align="center"><font size="6">Gang YU (俞刚)</font></p>
    <p align="justify">
                    I am a Research Leader for the Detection Team at <a href="https://megvii.com/">Megvii (Face++)</a>. My research interests focus on the computer vision and artificical intelligence, specifically on the topic of object detection, segmentation, human keypoint, and human action recognition. I obtained PhD from <a href="http://www.ntu.edu.sg/Pages/home.aspx">NTU</a> in 2014 supervised by Prof. <a target="_blank" href="https://www.cse.buffalo.edu/~jsyuan/index.html">Junsong Yuan</a>.
        <br/> <br/>
    </p>
    <p align=center>
      <a href="https://scholar.google.com.sg/citations?user=BJdigYsAAAAJ&hl=en">Google Scholar</a> /
                  <a href="CV.pdf">CV</a>
    </p>
              </td>
              <td width="20%">
              <br>
              <br>
    <img width="210"
                     src="IMGP9957.jpg" 
                     alt="Gang Yu">
        </td>
            </tr> 
          </table>

<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
            <tr>
              <td width="100%" align="left">
                <font size="5">News</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->
                  <p>
                  Our team obtained the first place in <strong><a  target="_blank" href="https://www.kaggle.com/c/cvpr-2018-autonomous-driving/leaderboard">Video Instance Segmentation</a></strong> [<a  target="_blank" href="">Report</a>] in the <a  target="_blank" href="http://www.wad.ai/index.html">WAD2018 (Workshop on Autonomous Driving)</a> Challenge.
                  </p>
                  <p>
                  Our team obtained the first place in <strong><a  target="_blank" href="http://activity-net.org/challenges/2018/tasks/guest_ava.html">AVA</a></strong> [<a  target="_blank" href="">Report</a>] and second place in <strong><a  target="_blank" href="http://moments.csail.mit.edu/results2018.html">Moments in time</a></strong> [<a  target="_blank" href="http://moments.csail.mit.edu/challenge2018/Megvii.pdf">Report</a>] in the <a  target="_blank" href="http://activity-net.org/challenges/2018/index.html">ActivityNet2018</a> Challenge.
                  </p>
                  <p>
                  Our team obtained the first place in <strong>COCO 2017 Challenge</strong> (Detection Track & Keypoint Track). <br>
<a href="http://cocodataset.org/#detections-leaderboard">http://cocodataset.org/#detections-leaderboard</a>   <br>
<a href="http://cocodataset.org/#keypoints-leaderboard">http://cocodataset.org/#keypoints-leaderboard</a>        <br>      
                  </p>
              </td>
            </tr>
          </table>


<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
            <tr>
              <td width="100%" align="left">
                <font size="5">Pre-prints</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->   
              </td>
            </tr>
<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1711.07264.pdf">Light-Head R-CNN: In Defense of Two-Stage Object Detector</a> [<a target="_blank" href="https://github.com/zengarden/light_head_rcnn">Code</a>]</b><br>
      Zeming Li, Chao Peng, <strong>Gang Yu</strong>, Xiangyu Zhang, Yangdong Deng, Jian Sun <br>                   
    <i>Arxiv</i>, 2017 <br>
    </p>
    </td>
    </tr>

  <tr>
    <td width="100%" align="left">
    <p>
    <b><a target="_blank" href="https://arxiv.org/pdf/1804.06215.pdf">DetNet: A Backbone network for Object
Detection
</a> </b><br>
      Zeming Li, Chao Peng, <strong>Gang Yu</strong>, Xiangyu Zhang, Yangdong Deng, Jian Sun <br>                   
    <i>Arxiv</i>, 2018 <br>
    </p>
    </td>
    </tr>

<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1804.06559.pdf">SFace: An Efficient Network for Face Detection in Large Scale Variations
</a></b><br>
      Jianfeng Wang, Ye Yuan, Boxun Li, <strong>Gang Yu</strong>, Jian Sun<br>                   
    <i>Arxiv</i>, 2018 <br>
    </p>
    </td>
    </tr>

  
<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1711.07246.pdf">Face Attention Network: An Effective Face Detector for the Occluded Faces
</a></b><br>
      Jianfeng Wang, Ye Yuan, <strong>Gang Yu</strong><br>                   
    <i>Arxiv</i>, 2017 <br>
    </p>
    </td>
    </tr>
  
  <tr>
    <td width="100%" align="left">
    <p>
    <b><a  target="_blank" href="https://arxiv.org/pdf/1805.00123.pdf">CrowdHuman: A Benchmark for Detecting Human in a Crowd</a> [<a target="_blank" href="https://sshao0516.github.io/CrowdHuman/">Project</a>]</b><br>
      Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, <strong>Gang Yu</strong>, Xiangyu Zhang, Jian Sun <br>                   
    <i>Arxiv</i>, 2018 <br>
    </p>
    </td>
    </tr>


          </table>

 
<br>
<br>
<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
    <tr>
      <td width="100%" align="left">
                <font size="5">Conference</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->                  
         </td>
       </tr>


    <tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1711.07240.pdf">MegDet: A Large Mini-Batch Object Detector</a></b><br>
      Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, <strong>Gang Yu</strong>, Jian Sun <br>                   
    <i>CVPR</i>, 2018<br>
    </p>
    </td>
    </tr>

<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1711.07319.pdf">Cascaded Pyramid Network for Multi-Person Pose Estimation</a> [<a target="_blank" href="https://github.com/chenyilun95/tf-cpn">Code</a>]</b><br>
      Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, <strong>Gang Yu</strong>, Jian Sun <br>                   
    <i>CVPR</i>, 2018 <br>
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://arxiv.org/pdf/1804.09337.pdf">Learning a Discriminative Feature Network for Semantic Segmentation</a></b><br>
      Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, <strong>Gang Yu</strong>, Nong Sang<br>                   
    <i>CVPR</i>, 2018<br>
    </p>
    </td>
    </tr>

       <tr>
         <td width="100%" valign="top">
    <p>
    <b><a href="https://arxiv.org/">R-FCN++: Towards Accurate Region-based Fully Convolutional Networks for Object Detection</a></b><br>
      Zeming Li, Yilun Chen, <strong>Gang Yu</strong>, Xiangyu Zhang, Jian Sun <br>
    <i>AAAI</i>, 2018 <br>                   
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://arxiv.org/pdf/1703.02719.pdf">Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network</a></b><br>
      Chao Peng, Xiangyu Zhang, <strong>Gang Yu</strong>, Guiming Luo, Jian Sun <br>
    <i>CVPR</i>, 2017 <br>                   
    </p>
    </td>
    </tr>    


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="http://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2015/Fast%20Action%20Proposals%20for%20Human%20Action%20Detection%20and%20Search.pdf">Fast Action Proposals for Human Action Detection and Search</a></b><br>
      <strong>Gang Yu</strong>, Junsong Yuan<br>
    <i>CVPR</i>, 2015 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://dl.dropboxusercontent.com/u/28637011/Paper/RGBD_ActionRecognition.pdf">Discriminative Orderlet Mining For Real-time Recognition of Human-Object Interaction</a></b>   [<a href="https://sites.google.com/site/skicyyu/rgbd_recognition">Project</a>]
    <br><strong>Gang Yu</strong>, Zicheng Liu, Junsong Yuan<br>
    <i>ACCV</i>, 2014 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2014/Scalable%20Forest%20Hashing%20for%20Fast%20Similarity%20Search.pdf">Scalable Forest Hashing for Fast Similarity Search</a></b> 
    <br><strong>Gang Yu</strong>, Junsong Yuan<br>
    <i>ICME</i>, 2014 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://pdfs.semanticscholar.org/b870/3f49d5ac4765b10b5707fea024f9faad931e.pdf">Propagative Hough Voting for Human Activity Recognition</a></b>
    <br><strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu<br>
    <i>ECCV</i>, 2012 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://pdfs.semanticscholar.org/1b38/aac66dbcf13e6d0cc60519b963aac323ef8f.pdf">Randomized Spatial Partition for Scene Recognition</a></b>
    <br>Yuning Jiang, Junsong Yuan, <strong>Gang Yu</strong><br>
    <i>ECCV</i>, 2012 <br>                   
    </p>
    </td>
    </tr>

    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://pdfs.semanticscholar.org/bbc1/1f20ad5ee2443bdce116e20177b27c96a7e5.pdf">Predicting Human Activities using Spatio-Temporal Structure of Interest Points</a></b>
    <br><strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu<br>
    <i>ACM MM</i>, 2012 <br>                   
    </p>
    </td>
    </tr>


    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/05995488-CVPR2011.pdf">Unsupervised Random Forest Indexing for Fast Action Search</a></b>
    <br><strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu<br>
    <i>CVPR</i>, 2011<br>                   
    </p>
    </td>
    </tr>
 

    <tr>
    <td width="100%" valign="top">
    <p>
    <b><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ACMMM2011-publish.pdf">Real-time HumanAction Search using Random Forest based Hough Voting</a></b>
    <br><strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu<br>
    <i>ACM MM</i>, 2011<br>                   
    </p>
    </td>
    </tr>
      
          </table> 


<br></br>

<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
            <tr>
              <td width="100%" align="left">
                <font size="5">Journal</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->   
              </td>
            </tr>
<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2015/Propagative%20Hough%20Voting%20for%20Human%20Activity%20Detection%20and%20Recognition.pdf">Propagative Hough Voting for Human Activity Detection and Recognition</a></b><br>
    <strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu <br>                   
    <i>IEEE Trans. on Circuits and Systems for Video Technology, Vol.25, Issue 1, pp.87-98</i>, 2014 <br>
    </p>
    </td>
    </tr>


<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="http://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2012/Action%20Search%20by%20Example%20using%20Randomized%20Visual%20Vocabularies.pdf">Action Search by Example using Randomized Visual Vocabularies</a></b><br>
    <strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu <br>                   
    <i>IEEE Trans. on Image Processing, Vol.22, Issue 1, pp. 377-390</i>, 2013 <br>
    </p>
    </td>
    </tr>



<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/05730498-TMM2011.pdf">Fast Action Detection via Discriminative Random Forest Voting and Top-K Subvolume Search</a></b><br>
    <strong>Gang Yu</strong>,  Norberto A., Junsong Yuan, Zicheng Liu <br>                   
    <i>IEEE Trans. on Multimedia, Vol.13, Issue 3, pp. 507-517</i>, 2013 <br>
    </p>
    </td>
    </tr>


          </table>






<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
            <tr>
              <td width="100%" align="left">
                <font size="5">Book</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->   
              </td>
            </tr>
<tr>
    <td width="100%" align="left">
    <p>
    <b><a href="https://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2015/Propagative%20Hough%20Voting%20for%20Human%20Activity%20Detection%20and%20Recognition.pdf">Human Action Analysis with Randomized Trees</a></b><br>
    <strong>Gang Yu</strong>, Junsong Yuan, Zicheng Liu <br>                   
    <i>SpringerBriefs, Springer</i>, 2014 <br>
    </p>
    </td>
    </tr>
    </table>


<table width="100%" align="left" border="0" cellspacing="0" cellpadding="5">
            <tr>
              <td width="100%" align="left">
                <font size="5">Links</font> <!-- <font size="3">(for full publications, please see google scholars)</font> -->   
              </td>
            </tr>
<tr>
    <td width="100%" align="left">
    <p>
    Team member: <a target='_blank' href='https://pcsci.github.io'>Chao Peng(彭超)</a>， <a target='_blank' href='http://www.zemingli.com/'>Zeming Li(黎泽明)</a>
    </p>
    </td>
    </tr>
    </table>
 
  
